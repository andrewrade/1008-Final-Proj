{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_array_statistics(url, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Calculate running mean and standard deviation for a large memory-mapped array\n",
    "    using chunked processing to manage memory usage.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    url : str\n",
    "        Path to the .npy file\n",
    "    chunk_size : int, optional\n",
    "        Number of temporal samples to process at once. If None, will use 1000 * temporal_dimension\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (running_mean, running_std) arrays of shape (channels,)\n",
    "    \"\"\"\n",
    "    # Open memory-mapped array\n",
    "    npy_array = np.lib.format.open_memmap(url, mode=\"r\")\n",
    "    total_samples, temporal, channels, height, width = npy_array.shape\n",
    "    \n",
    "    # Calculate effective batch size\n",
    "    merged_batch_size = total_samples * temporal\n",
    "    \n",
    "    # Set default chunk size if not provided\n",
    "    if chunk_size is None:\n",
    "        chunk_size = 1000 * temporal\n",
    "    \n",
    "    # Initialize statistics trackers with explicit dtype\n",
    "    running_mean = np.zeros(channels, dtype=np.float64)\n",
    "    running_var = np.zeros(channels, dtype=np.float64)\n",
    "    total_elements = 0\n",
    "    \n",
    "    # Process data in chunks\n",
    "    for start in range(0, merged_batch_size, chunk_size):\n",
    "        end = min(start + chunk_size, merged_batch_size)\n",
    "        \n",
    "        # Convert indices to sample and temporal ranges\n",
    "        sample_start = start // temporal\n",
    "        sample_end = (end - 1) // temporal + 1\n",
    "        temporal_offset_start = start % temporal\n",
    "        temporal_offset_end = end % temporal if end % temporal != 0 else temporal\n",
    "        \n",
    "        # Load and slice chunk\n",
    "        chunk = npy_array[sample_start:sample_end]\n",
    "        if sample_start == sample_end - 1:\n",
    "            chunk = chunk[:, temporal_offset_start:temporal_offset_end]\n",
    "        \n",
    "        # Reshape for efficient computation\n",
    "        reshaped = chunk.reshape(-1, channels, height, width).transpose(1, 0, 2, 3).reshape(channels, -1)\n",
    "        \n",
    "        # Update statistics using Welford's online algorithm with explicit type conversion\n",
    "        chunk_mean = np.asarray(reshaped.mean(axis=1), dtype=np.float64)\n",
    "        chunk_var = np.asarray(reshaped.var(axis=1), dtype=np.float64)\n",
    "        chunk_elements = float(reshaped.shape[1])  # Convert to float for division\n",
    "        \n",
    "        \n",
    "        total_elements += chunk_elements\n",
    "        delta = chunk_mean - running_mean\n",
    "        running_mean += delta * (chunk_elements / total_elements)\n",
    "        running_var += (chunk_var * (chunk_elements / total_elements) + \n",
    "                        (delta**2) * (chunk_elements * (total_elements - chunk_elements)) / \n",
    "                        total_elements**2)\n",
    "    \n",
    "    running_std = np.sqrt(running_var)\n",
    "    return running_mean, running_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(url, chunk_size=None):\n",
    "    \"\"\"\n",
    "    Calculate and print channel-wise statistics with proper formatting.\n",
    "    \"\"\"\n",
    "    means, stds = calculate_array_statistics(url, chunk_size)\n",
    "    \n",
    "    print(\"\\nChannel Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Channel':<10} {'Mean':>15} {'Std Dev':>15}\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "        print(f\"{i:<10} {mean:>15.6f} {std:>15.6f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Channel Statistics:\n",
      "--------------------------------------------------\n",
      "Channel               Mean         Std Dev\n",
      "--------------------------------------------------\n",
      "0                 0.000237        0.003330\n",
      "1                 0.009316        0.028159\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "url = r\"C:\\Users\\Andrew Deur\\Documents\\NYU\\DS-GA 1008 Deep Learning\\1008-Final-Proj\\data\\states.npy\"\n",
    "print_statistics(url, chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Channel Statistics:\n",
      "--------------------------------------------------\n",
      "Channel               Mean         Std Dev\n",
      "--------------------------------------------------\n",
      "0                 0.000237        0.008225\n",
      "1                 0.009316        0.069441\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_statistics(url, chunk_size=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
